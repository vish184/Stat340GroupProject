knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
player_stats_2019_20 <- read.csv("2019-20_player_stats.csv")
player_salaries_2019_20 <- read.csv("2019-20_player_salaries.csv")
#player_stats_2019_20
colnames(player_salaries_2019_20)[3] <- "Player"
#player_salaries_2019_20
player_stats_2020_21 <- read.csv("2020-21_player_stats.csv")
player_salaries_2020_21 <- read.csv("2020-21_player_salaries.csv")
#player_stats_2020_21
colnames(player_salaries_2020_21)[3] <- "salary"
#player_salaries_2020_21
merged_2019_20 <- merge(player_salaries_2019_20, player_stats_2019_20, by = 'Player')
merged_2019_20 <- merged_2019_20 %>%
distinct(Player, .keep_all = TRUE)
merged_2020_21 <- merge(player_salaries_2020_21, player_stats_2020_21, by = 'Player')
merged_2020_21 <- merged_2020_21 %>%
distinct(Player, .keep_all = TRUE)
data_split <- sort(sample(nrow(merged_2019_20), nrow(merged_2019_20)*0.8))
train <- merged_2019_20[data_split,]
dev <- merged_2019_20[-data_split,]
test <- merged_2020_21
model <- lm(salary ~ 1 + Age + G + GS + BLK + TOV + PF + PTS, train)
summary(model)
plot(model, which = 1)
plot(model, which = 3)
plot(model, which = 2)
library(car)
library(lmtest)
vcv0 <- hccm(model, type = "hc0")
vcv0
library("broom")
modelv2 <- tidy(coeftest(model, vcov = vcv0))
library(glmnet)
library(glmnet)
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
View(train)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
hridge_model <- hridge(y, X, 0.001, weights = weights)
hridge_model <- hridge(y, X, 0.001, weights)
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, 0.001, weights)
rsq_hridge_0001 <- hridge_model$rsq
hridge_model <- hridge(y, X, 0.001, weights)
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda, standardize = FALSE)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, 0.001, weights)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
lambda = 0.001
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
lambda = 0.001
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
rsq_hridge_0001 <- hridge_model$rsq
View(rsq_hridge_0001)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
lambda = 0.001
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
cv_model <- cv.glmnet(X, y, alpha = 0)
best_lambda <- cv_model$lambda.min
model_init <- glmnet(X, y, alpha = 0, lambda = best_lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
rsq_hridge_0001 <- hridge_model$rsq
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
lambda = 0.001
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
cv_model <- cv.glmnet(X, y, alpha = 0)
best_lambda <- cv_model$lambda.min
print(best_lambda)
model_init <- glmnet(X, y, alpha = 0, lambda = best_lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
rsq_hridge_0001 <- hridge_model$rsq
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
lambda = 546732
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
uni_model <- lm(y ~ X[, predictor])
coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}
# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
# Use regular ridge regression coefficient as initial values for optimization
cv_model <- cv.glmnet(X, y, alpha = 0)
best_lambda <- cv_model$lambda.min
print(best_lambda)
model_init <- glmnet(X, y, alpha = 0, lambda = best_lambda, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
# Solve optimization problem to get coefficients
coef <- optim(betas_init, hridge_loss)$par
# Compute fitted values and multiple R-squared
fitted <- X %*% coef
rsq <- cor(y, fitted)^2
names(coef) <- colnames(X)
output <- list("coef" = coef,
"fitted" = fitted,
"rsq" = rsq)
return(output)
}
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
rsq_hridge_0001 <- hridge_model$rsq
View(rsq_hridge_0001)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
player_stats_2019_20 <- read.csv("2019-20_player_stats.csv")
player_salaries_2019_20 <- read.csv("2019-20_player_salaries.csv")
#player_stats_2019_20
colnames(player_salaries_2019_20)[3] <- "Player"
#player_salaries_2019_20
player_stats_2020_21 <- read.csv("2020-21_player_stats.csv")
player_salaries_2020_21 <- read.csv("2020-21_player_salaries.csv")
#player_stats_2020_21
colnames(player_salaries_2020_21)[3] <- "salary"
#player_salaries_2020_21
merged_2019_20 <- merge(player_salaries_2019_20, player_stats_2019_20, by = 'Player')
merged_2019_20 <- merged_2019_20 %>%
distinct(Player, .keep_all = TRUE)
merged_2020_21 <- merge(player_salaries_2020_21, player_stats_2020_21, by = 'Player')
merged_2020_21 <- merged_2020_21 %>%
distinct(Player, .keep_all = TRUE)
data_split <- sort(sample(nrow(merged_2019_20), nrow(merged_2019_20)*0.8))
train <- merged_2019_20[data_split,]
dev <- merged_2019_20[-data_split,]
test <- merged_2020_21
model <- lm(salary ~ 1 + Age + G + GS + BLK + TOV + PF + PTS, train)
summary(model)
plot(model, which = 1)
plot(model, which = 3)
plot(model, which = 2)
library(car)
library(lmtest)
vcv0 <- hccm(model, type = "hc0")
vcv0
library("broom")
modelv2 <- tidy(coeftest(model, vcov = vcv0))
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
model5 <- glmnet(x, y, alpha = 0)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
model5 <- glmnet(X, y, alpha = 0)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
model5 <- glmnet(X, y, alpha = 0)
cv_model <- cv.glmnet(X, y, alpha = 0)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
model5 <- glmnet(X, y, alpha = 0)
cv_model <- cv.glmnet(X, y, alpha = 0)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_model <- glmnet(X, y, alpha = 0, lambda = best_lambda)
y_predicted <- predict(best_model, s = best_lambda, newx = X)
#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)
#find R-Squared
rsq <- 1 - sse/sst
rsq
[1] 0.7999513
library(glmnet)
y <- train$salary
X <- data.matrix(train[, c('Age', 'G', 'GS', 'BLK', 'TOV', 'PF', 'PTS')])
model5 <- glmnet(X, y, alpha = 0)
cv_model <- cv.glmnet(X, y, alpha = 0)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_model <- glmnet(X, y, alpha = 0, lambda = best_lambda)
y_predicted <- predict(best_model, s = best_lambda, newx = X)
#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)
#find R-Squared
rsq <- 1 - sse/sst
rsq
